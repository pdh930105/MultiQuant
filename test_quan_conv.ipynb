{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "__all__ = ['QConv', 'QConv1x1', 'QConv3x3', 'SwitchableBatchNorm2d', 'QLinear']\n",
    "\n",
    "\n",
    "class STE_discretizer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_in, num_levels):\n",
    "        x = x_in * (num_levels - 1)\n",
    "        x = torch.round(x)\n",
    "        x_out = x / (num_levels - 1)\n",
    "        return x_out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        return g, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv_Tra_Mulit(nn.Conv2d):\n",
    "    def  __init__(self, in_channels, out_channels, kernel_size, args, stride=1, padding=0, dilation=1, groups=1,\n",
    "                 bias=True, basewidth=0, expand_groups=0, oneBit_outchannel=-1, oneBit_inchannel=-1,\n",
    "                 last_conv=False, first_conv=False):\n",
    "        super(QConv_Tra_Mulit, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "\n",
    "        self.quan_weight = args.QWeightFlag\n",
    "        self.quan_act = args.QActFlag\n",
    "\n",
    "        if self.quan_weight:\n",
    "            self.weight_levels = -1\n",
    "            self.uW = nn.ParameterList([nn.Parameter(data=torch.tensor(0).float()) for _ in range(4)])\n",
    "            self.lW = nn.ParameterList([nn.Parameter(data=torch.tensor(0).float()) for _ in range(4)])\n",
    "\n",
    "        if self.quan_act:\n",
    "            self.act_levels = -1\n",
    "            self.uA = nn.ParameterList(\n",
    "                [nn.Parameter(data=torch.tensor([0, 0, 0, 0, 0, 0, 0]).float()) for _ in range(4)])\n",
    "            self.lA = nn.ParameterList(\n",
    "                [nn.Parameter(data=torch.tensor([0, 0, 0, 0, 0, 0, 0]).float()) for _ in range(4)])\n",
    "\n",
    "        self.weight_bit = 2  # defalut\n",
    "        self.act_bit = 2  # defalut\n",
    "        self.register_buffer('init', torch.tensor([0]))\n",
    "        self.STE_discretizer = STE_discretizer.apply\n",
    "\n",
    "        self.last_conv = last_conv\n",
    "        self.first_conv = first_conv\n",
    "        self.basewidth = basewidth\n",
    "        self.oneBit_outchannel = oneBit_outchannel\n",
    "        assert not self.last_conv or not self.first_conv\n",
    "\n",
    "    def weight_quantization(self, weight, group_index):\n",
    "        if not self.quan_weight or self.weight_bit == 32:\n",
    "            return weight\n",
    "\n",
    "        self.weight_levels = 2 ** self.weight_bit\n",
    "        weight = (weight - self.lW[group_index]) / (self.uW[group_index] - self.lW[group_index])\n",
    "        weight = weight.clamp(min=0, max=1)  # [0, 1]\n",
    "        weight = self.STE_discretizer(weight, self.weight_levels)\n",
    "        weight = (weight - 0.5) * 2  # [-1, 1]\n",
    "        return weight\n",
    "\n",
    "    def act_quantization(self, x, group_index):\n",
    "        if not self.quan_act or self.act_bit == 32:\n",
    "            return x\n",
    "\n",
    "        index = self.act_bit - 2\n",
    "        self.act_levels = 2 ** self.act_bit\n",
    "        # self.act_levels = 2 ** 8\n",
    "        x = (x - self.lA[group_index][index]) / (self.uA[group_index][index] - self.lA[group_index][index])\n",
    "        x = x.clamp(min=0, max=1)  # [0, 1]\n",
    "        x = self.STE_discretizer(x, self.act_levels)\n",
    "        return x\n",
    "\n",
    "    def select(self, weight, act):\n",
    "        if self.first_conv:\n",
    "            if self.weight_bit & 1 == 0:\n",
    "                if self.weight_bit == 2 or self.weight_bit == 8:\n",
    "                    act = act[:, :(self.weight_bit // 2) * self.basewidth, :, :]\n",
    "                    return weight[:act.shape[1], :, :, :], act\n",
    "                elif self.weight_bit == 4 or self.weight_bit == 6:\n",
    "                    act = act[:, self.basewidth: self.basewidth + (self.weight_bit // 2) * self.basewidth, :, :]\n",
    "                    return weight[self.basewidth: self.basewidth + act.shape[1], :, :, :], act\n",
    "            else:\n",
    "                if self.weight_bit == 3:\n",
    "                    act = act[:, :(self.weight_bit // 2) * self.basewidth + self.oneBit_outchannel, :, :]\n",
    "                    return weight[:act.shape[1], :, :, :], act\n",
    "                elif self.weight_bit == 5:\n",
    "                    act = act[:, :(self.weight_bit // 2) * self.basewidth + self.oneBit_outchannel, :, :]\n",
    "                    return weight[self.basewidth: self.basewidth + act.shape[1]], act\n",
    "                elif self.weight_bit == 7:\n",
    "                    act = act[:, :(self.weight_bit // 2) * self.basewidth + self.oneBit_outchannel, :, :]\n",
    "                    return weight[:act.shape[1], :, :, :], act\n",
    "        else:\n",
    "            if self.weight_bit & 1 == 0:\n",
    "                if self.weight_bit == 2 or self.weight_bit == 8:\n",
    "                    return weight[:act.shape[1], :, :, :], act\n",
    "                elif self.weight_bit == 4 or self.weight_bit == 6:\n",
    "                    return weight[self.basewidth: self.basewidth + act.shape[1], :, :, :], act\n",
    "            else:\n",
    "                if self.weight_bit == 3:\n",
    "                    return weight[:act.shape[1], :, :, :], act\n",
    "                elif self.weight_bit == 5:\n",
    "                    return weight[self.basewidth: self.basewidth + act.shape[1]], act\n",
    "                elif self.weight_bit == 7:\n",
    "                    return weight[:act.shape[1], :, :, :], act\n",
    "\n",
    "    def conv(self, FPweight_1, FPact_1):\n",
    "        Qweight_1 = self.group_weight_quantization(FPweight_1)\n",
    "        Qact_1 = self.group_activation_quantization(FPact_1)\n",
    "\n",
    "        output = F.conv2d(Qact_1, Qweight_1, self.bias, self.stride, self.padding, self.dilation,\n",
    "                          Qweight_1.shape[0])  # Qweight_1.shape[0] -> groups\n",
    "        return output\n",
    "\n",
    "    def group_activation_quantization(self, FPact_1):\n",
    "        if (self.weight_bit & 1) == 0:\n",
    "            tmp_list = []\n",
    "            FPact_1 = FPact_1.chunk(self.weight_bit // 2, dim=1)\n",
    "            for i in range(len(FPact_1)):\n",
    "                tmp = self.act_quantization(FPact_1[i], i)\n",
    "                tmp_list.append(tmp)\n",
    "            Qact_1 = torch.cat(tmp_list, dim=1)\n",
    "            return Qact_1\n",
    "        else:\n",
    "            tmp_list = []\n",
    "            FPact_1, FPact_2 = FPact_1[:, :(self.weight_bit // 2) * self.basewidth, :, :],\\\n",
    "                               FPact_1[:, (self.weight_bit // 2) * self.basewidth:, :, :]\n",
    "            FPact_1 = FPact_1.chunk(self.weight_bit // 2, dim=1)\n",
    "            for i in range(len(FPact_1)):\n",
    "                tmp = self.act_quantization(FPact_1[i], i)\n",
    "                tmp_list.append(tmp)\n",
    "            Qact_1 = torch.cat(tmp_list, dim=1)\n",
    "            Qact_2 = self.act_quantization(FPact_2, self.weight_bit//2)\n",
    "            return torch.cat([Qact_1, Qact_2], dim=1)\n",
    "\n",
    "    def group_weight_quantization(self, FPweight_1):\n",
    "        if (self.weight_bit & 1) == 0:\n",
    "            tmp_list = []\n",
    "            FPweight_1 = FPweight_1.chunk(self.weight_bit // 2, dim=0)\n",
    "            for i, _ in enumerate(FPweight_1):\n",
    "                tmp = self.weight_quantization(FPweight_1[i], i)\n",
    "                tmp_list.append(tmp)\n",
    "            Qweight_1 = torch.cat(tmp_list, dim=0)\n",
    "            return Qweight_1\n",
    "        else:\n",
    "            tmp_list = []\n",
    "            FPweight_1, FPweight_2 = FPweight_1[:(self.weight_bit // 2) * self.basewidth],\\\n",
    "                                     FPweight_1[(self.weight_bit // 2) * self.basewidth:]\n",
    "            FPweight_1 = FPweight_1.chunk(self.weight_bit // 2, dim=0)\n",
    "            for i in range(len(FPweight_1)):\n",
    "                tmp = self.weight_quantization(FPweight_1[i], i)\n",
    "                tmp_list.append(tmp)\n",
    "            Qweight_1 = torch.cat(tmp_list, dim=0)\n",
    "            Qweight_2 = self.weight_quantization(FPweight_2, self.weight_bit // 2)\n",
    "            return torch.cat([Qweight_1, Qweight_2], dim=0)\n",
    "\n",
    "    def initialize(self, x):\n",
    "        FPweight, FPact = self.weight.detach(), x.detach()\n",
    "        FPweight_1, FPact_1 = self.select(FPweight, FPact)\n",
    "\n",
    "        index = self.act_bit - 2\n",
    "\n",
    "        if self.quan_weight:\n",
    "            if (self.weight_bit & 1) == 0:\n",
    "                FPweight_1 = FPweight_1.chunk(self.weight_bit // 2, dim=0)\n",
    "                for i in range(len(FPweight_1)):\n",
    "                    self.uW[i].data.fill_(FPweight_1[i].std() * 3.0)\n",
    "                    self.lW[i].data.fill_(-FPweight_1[i].std() * 3.0)\n",
    "            else:\n",
    "                FPweight_1, FPweight_2 = FPweight_1[:(self.weight_bit // 2) * self.basewidth], \\\n",
    "                                         FPweight_1[(self.weight_bit // 2) * self.basewidth:]\n",
    "\n",
    "                FPweight_1 = FPweight_1.chunk(self.weight_bit // 2, dim=0)\n",
    "                for i in range(len(FPweight_1)):\n",
    "                    self.uW[i].data.fill_(FPweight_1[i].std() * 3.0)\n",
    "                    self.lW[i].data.fill_(-FPweight_1[i].std() * 3.0)\n",
    "\n",
    "                self.uW[self.weight_bit // 2].data.fill_(FPweight_2.std() * 3.0)\n",
    "                self.lW[self.weight_bit // 2].data.fill_(-FPweight_2.std() * 3.0)\n",
    "\n",
    "        if self.quan_act:\n",
    "            if (self.weight_bit & 1) == 0:\n",
    "                FPact_1 = FPact_1.chunk(self.weight_bit // 2, dim=1)\n",
    "                for i in range(len(FPact_1)):\n",
    "                    self.uA[i][index].data.fill_(FPact_1[i].std() / math.sqrt(1 - 2 / math.pi) * 3.0)\n",
    "                    self.lA[i][index].data.fill_(FPact_1[i].min())\n",
    "\n",
    "            else:\n",
    "                FPact_1, FPact_2 = FPact_1[:, :(self.weight_bit // 2) * self.basewidth, :, :], \\\n",
    "                                   FPact_1[:, (self.weight_bit // 2) * self.basewidth:, :, :]\n",
    "\n",
    "                FPact_1 = FPact_1.chunk(self.weight_bit // 2, dim=1)\n",
    "                for i in range(len(FPact_1)):\n",
    "                    self.uA[i][index].data.fill_(FPact_1[i].std() / math.sqrt(1 - 2 / math.pi) * 3.0)\n",
    "                    self.lA[i][index].data.fill_(FPact_1[i].min())\n",
    "\n",
    "                self.uA[self.weight_bit // 2][index].data.fill_(FPact_2.std() / math.sqrt(1 - 2 / math.pi) * 3.0)\n",
    "                self.lA[self.weight_bit // 2][index].data.fill_(FPact_2.min())\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.init == 1:\n",
    "            self.initialize(x)\n",
    "\n",
    "        FPweight, FPact = self.weight, x\n",
    "        FPweight_1, FPact_1 = self.select(FPweight, FPact)\n",
    "\n",
    "        output = self.conv(FPweight_1, FPact_1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref. https://github.com/ricky40403/DSQ/blob/master/DSQConv.py#L18\n",
    "class QConv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, args, stride=1, padding=0, dilation=1, groups=1,\n",
    "                 bias=True, oneBit_outchannel=-1, oneBit_inchannel=-1, last_conv=False, first_conv=False):\n",
    "        super(QConv, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        self.quan_weight = args.QWeightFlag\n",
    "        self.quan_act = args.QActFlag\n",
    "        self.STE_discretizer = STE_discretizer.apply\n",
    "        self.weight_bit = 2  # defalut\n",
    "        self.act_bit = 2  # defalut\n",
    "        self.oneBit_outchannel = oneBit_outchannel\n",
    "        self.oneBit_inchannel = oneBit_inchannel\n",
    "        self.last_conv = last_conv\n",
    "        self.first_conv = first_conv\n",
    "\n",
    "        if self.quan_weight:\n",
    "            self.weight_levels = 2 ** 2\n",
    "            self.uW = nn.ParameterList([nn.Parameter(data=torch.tensor(0).float()) for _ in range(4)])\n",
    "            self.lW = nn.ParameterList([nn.Parameter(data=torch.tensor(0).float()) for _ in range(4)])\n",
    "            # not used\n",
    "            self.register_buffer('bkwd_scaling_factorW', torch.tensor(args.bkwd_scaling_factorW).float())\n",
    "\n",
    "        if self.quan_act:\n",
    "            self.act_levels = -1\n",
    "            self.uA = nn.ParameterList([nn.Parameter(data=torch.tensor([0, 0, 0, 0, 0, 0, 0]).float()) for _ in range(4)])\n",
    "            self.lA = nn.ParameterList([nn.Parameter(data=torch.tensor([0, 0, 0, 0, 0, 0, 0]).float()) for _ in range(4)])\n",
    "            # not used\n",
    "            self.register_buffer('bkwd_scaling_factorA', torch.tensor(args.bkwd_scaling_factorA).float())\n",
    "\n",
    "        self.register_buffer('init', torch.tensor([0]))\n",
    "\n",
    "        # not used\n",
    "        self.output_scale = nn.ParameterList([nn.Parameter(data=torch.tensor(1).float()) for _ in range(7)])\n",
    "        self.hook_Qvalues = False\n",
    "        self.buff_weight = None\n",
    "        self.buff_act = None\n",
    "        assert not self.last_conv or not self.first_conv\n",
    "\n",
    "    def weight_quantization(self, weight, group_index):\n",
    "        if not self.quan_weight or self.weight_bit == 32:\n",
    "            return weight\n",
    "\n",
    "        weight = (weight - self.lW[group_index]) / (self.uW[group_index] - self.lW[group_index])\n",
    "        weight = weight.clamp(min=0, max=1)  # [0, 1]\n",
    "        weight = self.STE_discretizer(weight, self.weight_levels)\n",
    "        weight = (weight - 0.5) * 2  # [-1, 1]\n",
    "        return weight\n",
    "\n",
    "    def act_quantization(self, x, group_index):\n",
    "        if not self.quan_act or self.act_bit == 32:\n",
    "            return x\n",
    "\n",
    "        index = self.act_bit - 2\n",
    "        self.act_levels = 2 ** self.act_bit\n",
    "        x = (x - self.lA[group_index][index]) / (self.uA[group_index][index] - self.lA[group_index][index])\n",
    "        x = x.clamp(min=0, max=1)  # [0, 1]\n",
    "        x = self.STE_discretizer(x, self.act_levels)\n",
    "        return x\n",
    "\n",
    "    def group_activation_quantization(self, FPact_1, FPact_2=None):\n",
    "        if (self.weight_bit & 1) == 0:\n",
    "            tmp_list = []\n",
    "            FPact_1 = FPact_1.chunk(self.weight_bit // 2, dim=1)\n",
    "            for i in range(len(FPact_1)):\n",
    "                tmp = self.act_quantization(FPact_1[i], i)\n",
    "                tmp_list.append(tmp)\n",
    "            Qact_1 = torch.cat(tmp_list, dim=1)\n",
    "            return Qact_1\n",
    "        else:\n",
    "            tmp_list = []\n",
    "            FPact_1 = FPact_1.chunk(self.weight_bit // 2, dim=1)\n",
    "            for i in range(len(FPact_1)):\n",
    "                tmp = self.act_quantization(FPact_1[i], i)\n",
    "                tmp_list.append(tmp)\n",
    "            Qact_1 = torch.cat(tmp_list, dim=1)\n",
    "            Qact_2 = self.act_quantization(FPact_2, self.weight_bit//2)\n",
    "            return Qact_1, Qact_2\n",
    "\n",
    "    def group_weight_quantization(self, FPweight_1, FPweight_2=None):\n",
    "        if (self.weight_bit & 1) == 0:\n",
    "            tmp_list = []\n",
    "            FPweight_1 = FPweight_1.chunk(self.weight_bit // 2, dim=0)\n",
    "            for i in range(len(FPweight_1)):\n",
    "                tmp = self.weight_quantization(FPweight_1[i], i)\n",
    "                tmp_list.append(tmp)\n",
    "            Qweight_1 = torch.cat(tmp_list, dim=0)\n",
    "            return Qweight_1\n",
    "        else:\n",
    "            tmp_list = []\n",
    "            FPweight_1 = FPweight_1.chunk(self.weight_bit // 2, dim=0)\n",
    "            for i in range(len(FPweight_1)):\n",
    "                tmp = self.weight_quantization(FPweight_1[i], i)\n",
    "                tmp_list.append(tmp)\n",
    "            Qweight_1 = torch.cat(tmp_list, dim=0)\n",
    "            Qweight_2 = self.weight_quantization(FPweight_2, self.weight_bit // 2)\n",
    "            return Qweight_1, Qweight_2\n",
    "\n",
    "\n",
    "    def initialize(self, x):\n",
    "        FPweight, FPact = self.weight.detach(), x.detach()\n",
    "        FPweight_1, FPweight_2, FPact_1, FPact_2 = self.select(FPweight, FPact)\n",
    "\n",
    "        index = self.act_bit - 2\n",
    "\n",
    "        if self.quan_weight:\n",
    "            if (self.weight_bit & 1) == 0:\n",
    "                FPweight_1 = FPweight_1.chunk(self.weight_bit // 2, dim=0)\n",
    "                for i in range(len(FPweight_1)):\n",
    "                    self.uW[i].data.fill_(FPweight[i].std() * 3.0)\n",
    "                    self.lW[i].data.fill_(-FPweight[i].std() * 3.0)\n",
    "\n",
    "            else:\n",
    "                FPweight_1 = FPweight_1.chunk(self.weight_bit // 2, dim=0)\n",
    "                for i in range(len(FPweight_1)):\n",
    "                    self.uW[i].data.fill_(FPweight[i].std() * 3.0)\n",
    "                    self.lW[i].data.fill_(-FPweight[i].std() * 3.0)\n",
    "\n",
    "                self.uW[self.weight_bit // 2].data.fill_(FPweight_2.std() * 3.0)\n",
    "                self.lW[self.weight_bit // 2].data.fill_(-FPweight_2.std() * 3.0)\n",
    "\n",
    "        if self.quan_act:\n",
    "            if (self.weight_bit & 1) == 0:\n",
    "                FPact_1 = FPact_1.chunk(self.weight_bit // 2, dim=1)\n",
    "                for i in range(len(FPact_1)):\n",
    "                    self.uA[i][index].data.fill_(FPact_1[i].std() / math.sqrt(1 - 2 / math.pi) * 3.0)\n",
    "                    self.lA[i][index].data.fill_(FPact_1[i].min())\n",
    "\n",
    "            else:\n",
    "                FPact_1 = FPact_1.chunk(self.weight_bit // 2, dim=1)\n",
    "                for i in range(len(FPact_1)):\n",
    "                    self.uA[i][index].data.fill_(FPact_1[i].std() / math.sqrt(1 - 2 / math.pi) * 3.0)\n",
    "                    self.lA[i][index].data.fill_(FPact_1[i].min())\n",
    "\n",
    "                self.uA[self.weight_bit // 2][index].data.fill_(FPact_2.std() / math.sqrt(1 - 2 / math.pi) * 3.0)\n",
    "                self.lA[self.weight_bit // 2][index].data.fill_(FPact_2.min())\n",
    "\n",
    "    def select(self, weight, act):\n",
    "        if self.first_conv: # only select output channel\n",
    "            index_o = self.out_channels // self.groups * (self.weight_bit // 2)\n",
    "            index_i_fir = self.in_channels // self.groups * (self.weight_bit // 2)\n",
    "            index_i_sec = self.in_channels // self.groups * ((self.weight_bit + 1) // 2)\n",
    "            single_len = self.out_channels // self.groups\n",
    "\n",
    "            if self.weight_bit == 2 or self.weight_bit == 8:\n",
    "                return weight[:index_o, :, :, :], None, act[:, :index_i_fir, :, :], None\n",
    "            elif self.weight_bit == 4 or self.weight_bit == 6:\n",
    "                return weight[single_len: single_len + index_o, :, :, :], None, \\\n",
    "                       act[:, single_len: single_len + index_i_fir, :, :], None\n",
    "\n",
    "            elif self.weight_bit == 3:\n",
    "                return weight[:index_o, :, :, :], weight[index_o:index_o + self.oneBit_outchannel, :, :, :], \\\n",
    "                       act[:, :index_i_fir, :, :], act[:, index_i_fir:index_i_sec, :, :]\n",
    "            elif self.weight_bit == 5:\n",
    "                return weight[single_len: single_len + index_o, :, :, :], \\\n",
    "                       weight[single_len + index_o:single_len + index_o + self.oneBit_outchannel, :, :, :], \\\n",
    "                       act[:, :index_i_fir, :, :], act[:, index_i_fir:index_i_sec, :, :]\n",
    "            elif self.weight_bit == 7:\n",
    "                return weight[:index_o, :, :, :], weight[index_o:index_o + self.oneBit_outchannel, :, :, :], \\\n",
    "                       act[:, :index_i_fir, :, :], act[:, index_i_fir:index_i_sec, :, :]\n",
    "\n",
    "        elif self.last_conv: # only select input channel\n",
    "            index_o = self.out_channels // self.groups * (self.weight_bit // 2)\n",
    "            index_i_fir = self.in_channels // self.groups * (self.weight_bit // 2)\n",
    "            index_i_sec = self.in_channels // self.groups * (self.weight_bit // 2) + self.oneBit_inchannel\n",
    "            single_len = self.out_channels // self.groups\n",
    "\n",
    "            if self.weight_bit == 2 or self.weight_bit == 8:\n",
    "                return weight[:index_o, :index_i_fir, :, :], None, act, None\n",
    "            elif self.weight_bit == 4 or self.weight_bit == 6:\n",
    "                return weight[single_len: single_len+index_o, :index_i_fir, :, :], None, act, None\n",
    "\n",
    "            elif self.weight_bit == 3:\n",
    "                return weight[:index_o, :, :, :], \\\n",
    "                       weight[index_o:index_o + self.out_channels // self.groups, :self.oneBit_inchannel, :, :], \\\n",
    "                       act[:, :index_i_fir, :, :], act[:, index_i_fir:, :, :],\n",
    "            elif self.weight_bit == 5:\n",
    "                return weight[single_len: single_len + index_o, :, :, :], \\\n",
    "                       weight[single_len + index_o: single_len + index_o + self.out_channels // self.groups, :self.oneBit_inchannel, :, :], \\\n",
    "                       act[:, :index_i_fir, :, :], act[:, index_i_fir:, :, :],\n",
    "            elif self.weight_bit == 7:\n",
    "                return weight[:index_o, :, :, :], \\\n",
    "                       weight[index_o:index_o + self.out_channels // self.groups, :self.oneBit_inchannel, :, :], \\\n",
    "                       act[:, :index_i_fir, :, :], act[:, index_i_fir:, :, :],\n",
    "\n",
    "        else:\n",
    "            index_o = self.out_channels // self.groups * (self.weight_bit // 2)\n",
    "            index_i_fir = self.in_channels // self.groups * (self.weight_bit // 2)\n",
    "            index_i_sec = self.in_channels // self.groups * (self.weight_bit // 2) + self.oneBit_inchannel\n",
    "            # single_len = self.in_channels // self.groups\n",
    "            single_len = self.out_channels // self.groups\n",
    "\n",
    "            if self.weight_bit == 2 or self.weight_bit == 8:\n",
    "                return weight[:index_o, :, :, :], None, act, None\n",
    "            elif self.weight_bit == 4 or self.weight_bit == 6:\n",
    "                return weight[single_len: single_len+index_o, :, :, :], None, act, None\n",
    "\n",
    "            elif self.weight_bit == 3:\n",
    "                return weight[:index_o, :, :, :], \\\n",
    "                       weight[index_o:index_o + self.oneBit_outchannel, :self.oneBit_inchannel, :, :], \\\n",
    "                       act[:, :index_i_fir, :, :], act[:, index_i_fir:, :, :],\n",
    "            elif self.weight_bit == 5:\n",
    "                return weight[single_len: single_len + index_o, :, :, :], \\\n",
    "                       weight[single_len + index_o: single_len + index_o + self.oneBit_outchannel, :self.oneBit_inchannel, :, :], \\\n",
    "                       act[:, :index_i_fir, :, :], act[:, index_i_fir:, :, :],\n",
    "            elif self.weight_bit == 7:\n",
    "                return weight[:index_o, :, :, :], \\\n",
    "                       weight[index_o:index_o + self.oneBit_outchannel, :self.oneBit_inchannel, :, :], \\\n",
    "                       act[:, :index_i_fir, :, :], act[:, index_i_fir:, :, :],\n",
    "\n",
    "    def conv(self, FPweight_1, FPweight_2, FPact_1, FPact_2, quantized=True):\n",
    "        if self.first_conv:\n",
    "            if FPweight_2 is None: # 2, 4, 6, 8\n",
    "\n",
    "                Qweight_1 = self.group_weight_quantization(FPweight_1)\n",
    "                Qact_1 = self.group_activation_quantization(FPact_1)\n",
    "\n",
    "                output = F.conv2d(Qact_1, Qweight_1, self.bias, self.stride, self.padding, self.dilation,\n",
    "                                  self.weight_bit // 2) # self.weight_bit // 2 :-> groups\n",
    "            else:\n",
    "\n",
    "                Qweight_1, Qweight_2 = self.group_weight_quantization(FPweight_1, FPweight_2)\n",
    "                Qact_1, Qact_2 = self.group_activation_quantization(FPact_1, FPact_2)\n",
    "\n",
    "\n",
    "                output_1 = F.conv2d(Qact_1, Qweight_1, self.bias, self.stride, self.padding, self.dilation,\n",
    "                                  self.weight_bit // 2) # self.weight_bit // 2 :-> groups\n",
    "                output_2 = F.conv2d(Qact_2, Qweight_2, self.bias, self.stride, self.padding, self.dilation,\n",
    "                                    1) # 1 :-> groups\n",
    "                output = torch.cat((output_1, output_2), dim=1)\n",
    "\n",
    "        elif self.last_conv:\n",
    "            if FPweight_2 is None:  # 2, 4, 6, 8\n",
    "\n",
    "                Qweight_1 = self.group_weight_quantization(FPweight_1)\n",
    "                Qact_1 = self.group_activation_quantization(FPact_1)\n",
    "\n",
    "\n",
    "                output = F.conv2d(Qact_1, Qweight_1, self.bias, self.stride, self.padding, self.dilation,\n",
    "                                  self.weight_bit // 2) # self.weight_bit // 2 :-> groups\n",
    "            else:\n",
    "\n",
    "                Qweight_1, Qweight_2 = self.group_weight_quantization(FPweight_1, FPweight_2)\n",
    "                Qact_1, Qact_2 = self.group_activation_quantization(FPact_1, FPact_2)\n",
    "\n",
    "\n",
    "                output_1 = F.conv2d(Qact_1, Qweight_1, self.bias, self.stride, self.padding, self.dilation,\n",
    "                                    self.weight_bit // 2) # self.weight_bit // 2 :-> groups\n",
    "                output_2 = F.conv2d(Qact_2, Qweight_2, self.bias, self.stride, self.padding, self.dilation,\n",
    "                                    1) # 1 :-> groups\n",
    "                output = torch.cat((output_1, output_2), dim=1)\n",
    "\n",
    "        else:\n",
    "            if FPweight_2 is None: # 2, 4, 6, 8\n",
    "\n",
    "                Qweight_1 = self.group_weight_quantization(FPweight_1)\n",
    "                Qact_1 = self.group_activation_quantization(FPact_1)\n",
    "\n",
    "                output = F.conv2d(Qact_1, Qweight_1, self.bias, self.stride, self.padding, self.dilation,\n",
    "                                  self.weight_bit // 2) # self.weight_bit // 2 :-> groups\n",
    "            else:\n",
    "                Qweight_1, Qweight_2 = self.group_weight_quantization(FPweight_1, FPweight_2)\n",
    "                Qact_1, Qact_2 = self.group_activation_quantization(FPact_1, FPact_2)\n",
    "\n",
    "                output_1 = F.conv2d(Qact_1, Qweight_1, self.bias, self.stride, self.padding, self.dilation,\n",
    "                                    self.weight_bit // 2)# self.weight_bit // 2 :-> groups\n",
    "                output_2 = F.conv2d(Qact_2, Qweight_2, self.bias, self.stride, self.padding, self.dilation,\n",
    "                                    1) # 1 :-> groups\n",
    "                output = torch.cat((output_1, output_2), dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.init == 1:\n",
    "            self.initialize(x)\n",
    "\n",
    "        FPweight, FPact = self.weight, x\n",
    "        FPweight_1, FPweight_2, FPact_1, FPact_2 = self.select(FPweight, FPact)\n",
    "\n",
    "        output = self.conv(FPweight_1, FPweight_2, FPact_1, FPact_2, quantized=True)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
